{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21512910",
   "metadata": {},
   "source": [
    "# Week 7 Demo; Data Cleaning Lab; Preprocessing with Pandas\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand common data cleaning tasks for AI and large datasets;  \n",
    "- Learn to handle missing values, identify and manage outliers, and apply essential data transformations using pandas;  \n",
    "- Prepare data for later analysis and machine learning applications;  \n",
    "- Build confidence in interpreting, assessing, and cleaning real-world data.\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Agenda\n",
    "\n",
    "1. Setup and Introduction\n",
    "2. Theoretical Overview; Why Data Cleaning Matters in Large Scale AI Datasets\n",
    "3. Demo; Identifying and Handling Missing Values\n",
    "4. Demo; Detecting and Addressing Outliers\n",
    "5. Demo; Data Type Conversion and Feature Transformation\n",
    "6. Practical Mini-Exercise; Clean a Realistic Sample Dataset\n",
    "7. Industry Case Study; Cleaning a Public AI Dataset Sample\n",
    "8. Troubleshooting Common Issues; Best Practices in Data Cleaning Workflows\n",
    "9. Reflection; Self-Assessment and Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Introduction\n",
    "\n",
    "Welcome to Week 7.  \n",
    "This session will focus on essential data cleaning tasks using pandas in Python; skills needed for all modern AI and data engineering roles.  \n",
    "You will load, inspect, and prepare data using industry-standard workflows used by tech companies, research teams, and open data consortia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a358de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display options for easier exploration\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.width\", 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92daa10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Why Data Cleaning is Critical for AI\n",
    "\n",
    "- Raw or collected data is rarely perfect;  \n",
    "- Issues such as missing data, inconsistent types, and outliers can skew AI models;  \n",
    "- Large scale datasets (like LAION-5B, COCO) are documented with known imperfections;\n",
    "- Cleaning ensures reliable, transparent analysis and reproducible results;\n",
    "- Complies with industry standards for responsible AI and data use.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Demo; Identifying and Handling Missing Values\n",
    "\n",
    "Let us start by loading a small sample dataset with common problems;  \n",
    "This simulates the kinds of issues found in open datasets used for AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with missing values\n",
    "data = {\n",
    "    \"image_id\": [1, 2, 3, 4, 5],\n",
    "    \"caption\": [\"A dog runs\", np.nan, \"Two cats sleep\", \"A bird flies\", \"\"],\n",
    "    \"score\": [0.98, 0.76, None, 1.0, 0.89]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686ee71",
   "metadata": {},
   "source": [
    "### Detecting missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daba65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary count of all missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8d60f",
   "metadata": {},
   "source": [
    "### Cleaning options\n",
    "\n",
    "- Remove rows with any missing values;  \n",
    "- Fill missing values with a placeholder or estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing rows\n",
    "df_cleaned = df.dropna()\n",
    "df_cleaned\n",
    "\n",
    "# Fill missing captions\n",
    "df[\"caption\"] = df[\"caption\"].replace(\"\", np.nan)\n",
    "df[\"caption\"] = df[\"caption\"].fillna(\"No caption provided\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef408b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Demo; Detecting and Addressing Outliers\n",
    "\n",
    "Outliers can distort analysis and model training;  \n",
    "Industry often uses simple quantile or z-score methods;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eaf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example outlier to the dataset\n",
    "df.loc[5] = [6, \"A noise example\", 500] # Clearly unrealistic score\n",
    "\n",
    "# Detect outliers using interquartile range\n",
    "q1 = df[\"score\"].quantile(0.25)\n",
    "q3 = df[\"score\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "outliers = df[(df[\"score\"] < lower_bound) | (df[\"score\"] > upper_bound)]\n",
    "outliers\n",
    "\n",
    "# Optionally remove outliers\n",
    "df_no_outliers = df[~((df[\"score\"] < lower_bound) | (df[\"score\"] > upper_bound))]\n",
    "df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36db9a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Demo; Data Type Conversion and Feature Transformation\n",
    "\n",
    "Correct data types improve efficiency and analytical accuracy;  \n",
    "String categories, numeric conversions, and normalization are standard steps;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df.dtypes\n",
    "\n",
    "# Convert columns as needed\n",
    "df[\"image_id\"] = df[\"image_id\"].astype(str)\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "\n",
    "# Min-Max normalization for score column\n",
    "score_min = df[\"score\"].min()\n",
    "score_max = df[\"score\"].max()\n",
    "df[\"score_normalized\"] = (df[\"score\"] - score_min) / (score_max - score_min)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b4e9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Mini-Exercise; Clean a Realistic Sample Dataset\n",
    "\n",
    "Download the supplied sample data ('sample_open_data.csv') from the course resources.  \n",
    "Perform the following tasks;  \n",
    "- Identify any missing values and handle appropriately  \n",
    "- Find and deal with at least one outlier  \n",
    "- Convert columns to correct data types  \n",
    "- Create a new normalized feature column ('score_scaled')  \n",
    "- Summarize your cleaning steps; explain your choices\n",
    "\n",
    "Use a new notebook cell for your solution.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Industry Case Study; Cleaning a Public AI Dataset Sample\n",
    "\n",
    "Many open datasets for AI (such as LAION-5B or COCO) require substantial cleaning before use;\n",
    "For example, in LAION image-caption data, missing captions or extreme scores are common issues;  \n",
    "- Review the short LAION sample data provided in the resources  \n",
    "- Apply your cleaning and transformation steps  \n",
    "- Discuss how these realistically impact downstream ML or AI work\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Troubleshooting Common Issues and Best Practices\n",
    "\n",
    "- Always check data types after cleaning;  \n",
    "- Document each cleaning step for reproducibility;  \n",
    "- Use assert statements to validate cleaning if applicable;  \n",
    "- Where possible, automate cleaning scripts for large datasets;  \n",
    "- Never delete or change original data; always work on a copy.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Reflection and Self-Assessment\n",
    "\n",
    "- What challenges did you encounter in the cleaning process?  \n",
    "- How do your cleaning decisions affect the reliability of later ML analysis?  \n",
    "- How might you automate these steps for much larger datasets?  \n",
    "- List two industry standards or best practices you followed.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Next week, you will learn about documenting, versioning, and storing data workflows;  \n",
    "These skills will ensure your cleaned datasets remain useful, reproducible, and ready for collaboration in real-world AI or data science teams."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
