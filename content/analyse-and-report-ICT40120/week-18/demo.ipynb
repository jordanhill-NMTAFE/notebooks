{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0728c0",
   "metadata": {},
   "source": [
    "# Week 18 Demo: Course Synthesis and Knowledge Test Preparation\n",
    "\n",
    "## 1. Setup and Introduction\n",
    "\n",
    "Welcome to Week 18. This session focuses on consolidating your learning and preparing you for the upcoming knowledge-based assessment. By the end of this workshop, you will;\n",
    "- Review foundational and advanced concepts covered throughout the course.\n",
    "- Practice with a mock knowledge test to identify gaps and reinforce critical knowledge areas.\n",
    "- Engage in hands-on coding exercises reflecting workplace tasks in data science and AI dataset engineering.\n",
    "- Participate in a Q&A and receive targeted coaching to maximize assessment readiness.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mock Knowledge Test\n",
    "\n",
    "Answer each of the following questions. Check the explanation after each answer to self-assess and identify gaps.\n",
    "\n",
    "### Q1. What is an open dataset and how does its structure benefit AI model development?\n",
    "\n",
    "*Write your answer below; then check the model answer.*\n",
    "\n",
    "**Model Answer**;  \n",
    "An open dataset is a collection of data available publicly, usually with minimal restrictions. Its standardized, well-documented structure ensures transparency and reproducibility, allowing AI models to be trained and evaluated consistently.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Name two common tools for large-scale data analysis in Python. What are their primary uses?\n",
    "\n",
    "*Write your answer below; then check the model answer.*\n",
    "\n",
    "**Model Answer**;  \n",
    "pandas is used for data manipulation and analysis with structured data (tables, CSV, DataFrames); numpy is used for efficient numerical computations on large arrays and matrices.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. Explain the importance of dataset documentation and version control in industry projects.\n",
    "\n",
    "*Write your answer below; then check the model answer.*\n",
    "\n",
    "**Model Answer**;  \n",
    "Thorough dataset documentation enables others to understand and use the dataset correctly; version control tracks changes, ensuring reproducibility and accountability across teams when working on evolving datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. Describe a real-world example where data ethics or privacy could directly impact an AI project.\n",
    "\n",
    "*Write your answer below; then check the model answer.*\n",
    "\n",
    "**Model Answer**;  \n",
    "Developing a facial recognition model using unconsented, scraped images from the web raises privacy violations; failing to address this could lead to legal issues and loss of public trust.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. Using the code snippet below, what does the function accomplish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"open_data.csv\")\n",
    "df_clean = df.dropna().drop_duplicates()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e6920",
   "metadata": {},
   "source": [
    "**Model Answer**;  \n",
    "The function reads a CSV into a DataFrame, then removes any rows with missing values and duplicates, finally printing the shape (rows, columns) of the cleaned dataset.\n",
    "\n",
    "---\n",
    "\n",
    "*Continue to use this self-test format for additional knowledge areas as needed before proceeding to coding review/demos.*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Industry Case Review and Mini-Demos\n",
    "\n",
    "### Review: Loading and Documenting a Dataset\n",
    "\n",
    "**Task**;  \n",
    "Load a public dataset (such as one from HuggingFace or UCI ML Repository); Generate a basic data dictionary describing column meanings and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: UCI Wine Quality Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, sep=';')\n",
    "data_dictionary = pd.DataFrame({\n",
    "    \"Column\": df.columns,\n",
    "    \"Type\": [df[col].dtype for col in df.columns],\n",
    "    \"Example_Value\": [df[col].iloc[0] for col in df.columns]\n",
    "})\n",
    "display(data_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f78d2",
   "metadata": {},
   "source": [
    "**Description**;  \n",
    "This code demonstrates fetching a real-world dataset, checking structure, and documenting key features, reflecting industry best practices for dataset onboarding.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo: Handling Missing Data\n",
    "\n",
    "**Task**;  \n",
    "Identify and report missing values, then impute missing data for numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c72d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = df.isnull().sum()\n",
    "print(\"Missing values per column;\", missing_summary)\n",
    "\n",
    "# Impute missing values (if any) using column mean\n",
    "df_imputed = df.fillna(df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d45e5",
   "metadata": {},
   "source": [
    "**Description**;  \n",
    "Reporting and imputing missing values reduces data quality issues and ensures sound machine learning outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo: Quick Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Task**;  \n",
    "Visualize feature distributions using industry-standard tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.hist(bins=20, figsize=(12, 8))\n",
    "plt.suptitle('Feature Distributions in Wine Quality Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4f021",
   "metadata": {},
   "source": [
    "**Description**;  \n",
    "This EDA step provides quick insights into feature ranges and potential outliers, supporting informed preprocessing for AI workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo: Simple ML Application (Classification)\n",
    "\n",
    "**Task**;  \n",
    "Apply logistic regression to predict wine quality category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Binary classification; quality >= 7 is 'good'\n",
    "df['good_quality'] = (df['quality'] >= 7).astype(int)\n",
    "\n",
    "X = df.drop(['quality', 'good_quality'], axis=1)\n",
    "y = df['good_quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test set accuracy;\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b6d35",
   "metadata": {},
   "source": [
    "**Description**;  \n",
    "This classification pipeline mirrors real world workplace tasks, from data preparation to model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Practical Exercises\n",
    "\n",
    "1. From a provided open dataset, document five potential sources of bias and propose one mitigation strategy for each.\n",
    "2. Implement a data versioning workflow using a tool such as DVC or Git-LFS; track changes to raw vs. cleaned data files and reflect on traceability in team settings.\n",
    "3. Formulate three key questions that a compliance officer would need answered to verify your project's adherence to data privacy legislation.\n",
    "4. Re-analyze a failed ML experiment from your previous project; identify the data or modelling issue, document it, and explain how you resolved it.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Reflection and Self-Assessment Prompts\n",
    "\n",
    "- List three ways that your Python and ML skills developed over the course connect to typical AI team workflows in industry.\n",
    "- Reflect on one ethical dilemma you encountered or discussed during the course, and describe how you would address it in a professional setting.\n",
    "- Identify an aspect of your data documentation that you would improve for an industry audience, explaining your reasoning.\n",
    "- What feedback on your project work so far has helped you most in developing your dataset analysis or reporting skills?\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Q&A and Final Assessment Checklist\n",
    "\n",
    "Open Q&A;  \n",
    "- Bring unresolved conceptual, technical, or project challenges for discussion.\n",
    "- Share practical workplace scenarios for instructor or peer feedback.\n",
    "\n",
    "Assessment Preparation Checklist;  \n",
    "- Ensure all competency requirements for units BSBDAT501, ICTDAT401, and BSBCRT404 are addressed in your submissions.\n",
    "- Review and complete all project documentation (including data dictionaries, version history, and ethical considerations).\n",
    "- Practice with additional knowledge review/test questions before formal assessment in Week 18.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "This session is your opportunity to consolidate learning, clarify assessment expectations, and resolve outstanding challenges. Effective preparation draws on course content, practical skills, and industry standards for AI dataset engineering. Next steps; complete final assessments, seek feedback proactively, and prepare for workplace-based application of your learning. Week 19 will focus on reassessment opportunities and final project support if required."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
