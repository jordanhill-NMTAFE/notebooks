{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34639f4",
   "metadata": {},
   "source": [
    "# Week 1 Demo; How Data Drives AI; Demonstrating Large Dataset Impact\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how dataset size and quality affect AI model performance.\n",
    "- Identify what open datasets are and how they are used in real AI projects.\n",
    "- Observe and discuss hands-on examples using real data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Welcome and Setup\n",
    "\n",
    "- Introduce course focus; real-world, large-scale AI datasets and practical skills.\n",
    "- Outline session activities; live demo, discussion, mini-coding lab.\n",
    "- Ensure every student can access Jupyter Notebooks and basic Python tools.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Data Matters in AI\n",
    "\n",
    "- AI learns patterns by analyzing data; larger, cleaner datasets = better results.\n",
    "- Modern systems need millions of examples (e.g., for language translation or image recognition).\n",
    "- Industry examples; OpenAI uses huge datasets to train chat models; image recognition in self-driving cars uses millions of pictures.\n",
    "- Start a quick group brainstorm; list AI you use every day that relies on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What Are Open Datasets?\n",
    "\n",
    "- Open datasets are freely accessible data collections; used by both companies and researchers.\n",
    "- Key examples include LAION-5B (images and text used for AI vision/language) and COCO (image labeling for ML).\n",
    "- Open datasets support transparency, reproducibility, and responsible AI.\n",
    "- Students review dataset websites briefly; note scale (billions of images/texts).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Demo; Small vs Large Dataset in Practice\n",
    "\n",
    "### Activity Overview\n",
    "\n",
    "- Use a simple, well-known dataset (MNIST digits or UCI Iris dataset).\n",
    "- Show what happens when you train a machine learning model on a small subset versus a larger subset of the same data.\n",
    "- Discuss real-world implications; self-driving car trained on 100 vs 1,000,000 images.\n",
    "- Allow students to predict what will happen; which model will perform better and why.\n",
    "\n",
    "### Code Example (Python, Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97dea6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Create small and large training sets\n",
    "X_small, _, y_small, _ = train_test_split(digits.data, digits.target, train_size=0.1, stratify=digits.target, random_state=42)\n",
    "X_large, _, y_large, _ = train_test_split(digits.data, digits.target, train_size=0.8, stratify=digits.target, random_state=42)\n",
    "\n",
    "# Define and train models\n",
    "model_small = LogisticRegression(max_iter=1000)\n",
    "model_large = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model_small.fit(X_small, y_small)\n",
    "model_large.fit(X_large, y_large)\n",
    "\n",
    "# Evaluate on test data\n",
    "_, X_test, _, y_test = train_test_split(digits.data, digits.target, test_size=0.2, stratify=digits.target, random_state=42)\n",
    "\n",
    "y_pred_small = model_small.predict(X_test)\n",
    "y_pred_large = model_large.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with small dataset:\", accuracy_score(y_test, y_pred_small))\n",
    "print(\"Accuracy with large dataset:\", accuracy_score(y_test, y_pred_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02fb46",
   "metadata": {},
   "source": [
    "- Students compare results; discuss why larger dataset improves results.\n",
    "- Visualize a few predictions with images to drive the point home.\n",
    "  \n",
    "---\n",
    "\n",
    "## 5. Industry Case Study; LAION-5B and COCO\n",
    "\n",
    "- Very large datasets like LAION-5B enable advanced AI (e.g., image search, text-to-image models like DALL-E).\n",
    "- Real projects; Google Photos and OpenAI models leverage millions/billions examples.\n",
    "- Discuss issues of bias, quality, and ethics (set up for future weeks).\n",
    "- Show a short video or slide of a modern AI system referencing open datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Mini Challenge\n",
    "\n",
    "- Students form pairs; brainstorm three examples in their daily life influenced by training data scale.\n",
    "- Volunteers share thoughts; facilitator highlights dataset role in each AI example.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Reflection and Next Steps\n",
    "\n",
    "- Discuss what surprised students most about demo results.\n",
    "- Ask; why might some data not improve a model? Introduce concept of data quality for Week 6.\n",
    "- Preview next week; where to find open data and how to judge its reliability.\n",
    "- Quick quiz; 3 questions testing understanding of why dataset scale matters.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "- Reinforce big idea; high-quality, large-scale data is the foundation of modern AI.\n",
    "- Highlight main tools and datasets to be explored through the course.\n",
    "- Invite students to explore dataset websites and bring one interesting dataset idea for discussion in Week 2."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
