{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b408778",
   "metadata": {},
   "source": [
    "# Week 16 Demo. Ethics Debate; Solving Complex Dilemmas in AI Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Identify and critically evaluate advanced ethical dilemmas in AI datasets.\n",
    "- Develop and present reasoned arguments to address real-world data ethics challenges.\n",
    "- Apply technical and procedural methods to mitigate privacy and bias risks in modern AI data workflows.\n",
    "- Enhance communication and debate skills relevant to industry and workplace settings.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction; Advanced Data Ethics Challenges in AI\n",
    "\n",
    "The use of very large and open datasets for machine learning raises unique and complex ethical questions not fully addressed by traditional data compliance frameworks. Key challenge areas include:\n",
    "- Dataset bias and fair representation of minority groups.\n",
    "- Informed consent and secondary data use.\n",
    "- Privacy risks in generative AI and LLM training.\n",
    "- Policy and compliance in global datasets.\n",
    "- Balancing data utility and ethical harm.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Case Scenarios; Real-World AI Data Dilemmas\n",
    "\n",
    "### Scenario 1; Dataset Bias in Face Recognition\n",
    "\n",
    "A public face recognition dataset is found to have low representation of certain ethnic groups. ML models trained on this data perform poorly for minorities, leading to unfair outcomes in real-world systems.\n",
    "\n",
    "- What bias mitigation strategies could you implement?\n",
    "- What are the risks if these biases are unaddressed?\n",
    "\n",
    "### Scenario 2; Informed Consent and Web-Scraped Data\n",
    "\n",
    "A large language model is trained on open web data, including social media posts and online forums. Many data subjects did not provide explicit permission for their content to be used in AI training.\n",
    "\n",
    "- Is this use ethical, even if technically legal?\n",
    "- What privacy-preserving solutions could address these concerns?\n",
    "\n",
    "### Scenario 3; Regulatory Change and Data Policy\n",
    "\n",
    "A national privacy law changes, making certain previously open datasets non-compliant. Your organization has an existing workflow relying on these datasets.\n",
    "\n",
    "- How do you respond to sudden policy changes?\n",
    "- What steps ensure ongoing compliance and responsible data management?\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hands-On Activity; Implementing Privacy-Preserving Demo\n",
    "\n",
    "You will now explore technical approaches to reduce privacy risks in dataset sharing and analysis.\n",
    "\n",
    "### Example; Data Anonymization with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset containing personally identifiable information\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": [101, 102, 103],\n",
    "    \"email\": [\"user1@email.com\", \"user2@email.com\", \"user3@email.com\"],\n",
    "    \"age\": [23, 34, 28],\n",
    "    \"gender\": [\"female\", \"male\", \"nonbinary\"]\n",
    "})\n",
    "\n",
    "# Remove direct identifiers for anonymization\n",
    "anonymized_df = df.drop(columns=[\"user_id\", \"email\"])\n",
    "print(anonymized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583b3d7",
   "metadata": {},
   "source": [
    "**Exercise.** Modify the above to implement k-anonymity by generalizing the 'age' feature (use ranges instead of exact ages).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Debate Setup; Roles, Rules, and Structure\n",
    "\n",
    "### Teams\n",
    "\n",
    "- Split class into teams. Each team will be assigned one ethical case scenario (see above).\n",
    "- Roles; Each team selects Lead Speaker, Researcher, and Challenger.\n",
    "\n",
    "### Preparation\n",
    "\n",
    "- Research real examples and industry standards relevant to your scenario. Explore how big tech, non-profits, or research labs responded to similar dilemmas.\n",
    "- Use course resources and last week's project tools to analyze your scenario, referencing data ethics frameworks (e.g., ACM, IEEE, GDPR).\n",
    "\n",
    "### Debate Structure\n",
    "\n",
    "1. Each team presents a summary of the case and their recommended ethical solution (up to 5 minutes).\n",
    "2. Opposing team delivers a rebuttal, identifying possible risks, unintended consequences, or weaknesses (3 minutes).\n",
    "3. Open Q&A; Other students ask clarifying questions.\n",
    "4. Final reflection; Teams summarize how their views evolved or solidified (2 minutes each).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Technical Exploration; Fairness Metrics with sklearn and Fairlearn\n",
    "\n",
    "Evaluate model fairness using open-source tools available in industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sample dataset\n",
    "data = fetch_openml(data_id=1590, as_frame=True) # 'adult' dataset\n",
    "X = data.data\n",
    "y = (data.target == '>50K').astype(int)\n",
    "\n",
    "# Use 'sex' as sensitive feature\n",
    "A = X[\"sex\"]\n",
    "\n",
    "# Simple pre-processing\n",
    "X = X.select_dtypes(include='number').fillna(0)\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "    X, y, A, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Train a basic classifier\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Check demographic parity difference\n",
    "dpd = demographic_parity_difference(y_test, y_pred, sensitive_features=A_test)\n",
    "print(\"Demographic Parity Difference:\", dpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32f063",
   "metadata": {},
   "source": [
    "**Exercise.** What could you do if your model shows a high demographic parity difference (>0.1)? List two mitigation strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Reflection and Assessment Preparation\n",
    "\n",
    "### Self-Check Questions\n",
    "\n",
    "- What are the main ethical risks when collecting and processing open datasets in AI?\n",
    "- How do policy and regulation changes affect ongoing AI projects?\n",
    "- When is it appropriate to use, anonymize, or avoid certain data in AI model training?\n",
    "- Prepare a short paragraph; Summarize one strategy to improve ethical outcomes in your course projects.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary and Next Steps\n",
    "\n",
    "- Ethical analysis is a critical skill for modern data science and AI professionals.\n",
    "- Practical technical tools and robust debate skills both contribute to responsible AI development.\n",
    "- Next week; Integrating your learning; Finalizing open dataset and ML projects."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
