{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbce1c44",
   "metadata": {},
   "source": [
    "# Week 5: Practical Tensor Manipulation with PyTorch\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the role of PyTorch and tensors in modern machine learning workflows\n",
    "- Create, index, reshape, and batch tensors in PyTorch\n",
    "- Connect tensor operations to real-world, industry workflows\n",
    "- Develop skills foundational for rapid prototyping, model development, and scalable ML on cloud/HPC\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction; The Role of PyTorch in Industry\n",
    "\n",
    "PyTorch is widely used for developing, training, and deploying machine learning and AI models. Its tensor library enables fast mathematical operations on CPU and GPU, making it especially powerful for high-performance and cloud-based ML workflows.\n",
    "\n",
    "- PyTorch is fundamental for machine learning engineers working on scalable, production-ready systems\n",
    "- Tensor manipulation forms the backbone of all training, inference, and data transformation tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environment Setup\n",
    "\n",
    "Before starting, ensure you have access to JupyterLab or VSCode and PyTorch installed (CPU or GPU). For cloud delivery, this may be on Azure or local Ubuntu VMs. Check installation with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cf651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Available?\" , torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9895a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Creating Tensors\n",
    "\n",
    "Tensors are multi-dimensional arrays, similar to NumPy arrays, but with GPU acceleration. Industry AI models use tensors to store data, parameters, gradients, and more.\n",
    "\n",
    "### Create various types of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor from a list\n",
    "tensor_example = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"2x2 Tensor;\")\n",
    "print(tensor_example)\n",
    "\n",
    "# Random tensor for initialising neural network weights\n",
    "weights = torch.rand(3, 3)\n",
    "print(\"3x3 Random Weight Matrix;\")\n",
    "print(weights)\n",
    "\n",
    "# Zero and one tensors, useful for data/batch initialization\n",
    "zeros_tensor = torch.zeros(2, 4)\n",
    "ones_tensor = torch.ones(4)\n",
    "print(zeros_tensor)\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19205181",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Indexing and Slicing Tensors\n",
    "\n",
    "Practical machine learning workflows require extracting and updating tensor values with high performance; for example, selecting batches or updating model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf349450",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(16).reshape(4, 4)\n",
    "print(\"Original Data;\")\n",
    "print(data)\n",
    "\n",
    "# Index single value\n",
    "print(\"Value at row 1, col 2;\", data[1, 2])\n",
    "\n",
    "# Slice a batch (rows 1 and 2)\n",
    "print(\"Rows 1 and 2;\")\n",
    "print(data[1:3])\n",
    "\n",
    "# Update a section (simulate gradient update step)\n",
    "data[0:2, 0:2] = 99\n",
    "print(\"After update;\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67783a21",
   "metadata": {},
   "source": [
    "**Practical tip:** Efficient slicing supports scalable data loading in production training.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Reshaping and Transposing Tensors\n",
    "\n",
    "Real-world ML tasks (e.g., computer vision, NLP) often require reshaping data for batch processing or neural network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd220ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.arange(24).reshape(2, 3, 4)  # Example: 2 batches, 3 instances, 4 features\n",
    "print(\"Original Shape;\", batch.shape)\n",
    "\n",
    "# Flatten all but batch for feeding into linear layer\n",
    "flattened = batch.view(2, -1)\n",
    "print(\"Flattened for NN input;\", flattened.shape)\n",
    "\n",
    "# Transpose dimensions (common in sequence models)\n",
    "transposed = batch.permute(1, 0, 2)\n",
    "print(\"Transposed Shape;\", transposed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126d491",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Batch Operations and Broadcasting\n",
    "\n",
    "Industry standard ML operations involve automatic expansion (broadcasting) to efficiently handle data of varying shapes – crucial for cloud scale ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e22dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.arange(3)\n",
    "# Broadcasting b to shape (2, 3)\n",
    "sum_result = a + b\n",
    "print(\"Broadcast Addition Result;\")\n",
    "print(sum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537e19b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hands-On Challenges\n",
    "\n",
    "### Exercise 1; Create a 3D tensor of shape (4, 3, 2) filled with random numbers\n",
    "### Exercise 2; Extract the second “row” from each “batch” of your tensor\n",
    "### Exercise 3; Reshape a (16,) tensor into a (4, 4) matrix, then transpose it\n",
    "### Exercise 4; Simulate a batch update by multiplying all elements in the first batch by 0.9\n",
    "### Exercise 5; Document one practical scenario in your own words where tensor reshaping is necessary in machine learning\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Industry Case Study; Why Tensor Manipulation Matters\n",
    "\n",
    "In a production AI pipeline, model training requires loading massive volumes of image data, batching into tensors, reshaping for GPU memory efficiency, and slicing for augmentations; these workflows enable companies to scale up deep learning pipelines on cloud VMs and HPC clusters.\n",
    "\n",
    "- “How does this relate to your work or future role?” Reflect on how precise tensor control enables scalable, reliable ML deployments\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Troubleshooting and Best Practices\n",
    "\n",
    "- If you get shape mismatch errors, print tensor shapes at each step\n",
    "- Use .clone() when making modified copies of tensors to avoid in-place changes affecting original data\n",
    "- Apply .to(device) to move tensors to GPU for large-scale acceleration\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Reflection and Assessment Preparation\n",
    "\n",
    "- Complete all code examples and exercises; compare your answers with peers for review\n",
    "- Document at least two “gotchas” or issues you encountered and how you solved them; this aligns with industry documentation standards\n",
    "- Prepare a 1-2 sentence summary; why is practical tensor manipulation essential for AI engineers working in industry?\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary and Next Steps\n",
    "\n",
    "- This session has established key skills for working with PyTorch tensors, a foundation for cloud ML, GPU acceleration, and distributed training\n",
    "- Review and annotate your code examples; this will form a portfolio component for Week 10 assessment\n",
    "- Next week we build on these foundations to start using PyTorch autograd and neural network modules"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
