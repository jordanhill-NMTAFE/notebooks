{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c04cf2f",
   "metadata": {},
   "source": [
    "# Week 17 Demo; Multi-GPU Distributed Training, Experiment Tracking, and Performance Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand principles and benefits of multi-GPU distributed training in PyTorch.\n",
    "- Set up and run distributed jobs using torchrun or accelerate.\n",
    "- Integrate experiment tracking tools to monitor training and analyse results.\n",
    "- Interpret, compare, and present distributed training performance metrics.\n",
    "- Prepare project results for technical and non-technical audiences.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction; Real-World Industry Context\n",
    "\n",
    "- High-performance ML teams routinely use distributed training to accelerate AI workloads; mastering this is key for production roles.\n",
    "- Experiment tracking ensures reproducibility and comparison of different models or runs, forming the basis for MLOps best practices.\n",
    "- Industry workflows demand code that is clear, well-tracked, and demonstrably scalable; this lab models those requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Review; From Single-GPU to Multi-GPU Workflows\n",
    "\n",
    "- In previous labs you implemented and ran PyTorch scripts on a single GPU; you learned to test, document, and report results.\n",
    "- This week you will expand your workflow; scaling the same model to run in parallel over multiple GPUs using data parallelism.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Setup; Environment and Data Preparation\n",
    "\n",
    "- Ensure your environment has access to at least two GPUs; this may be Azure VM or local workstation.\n",
    "- Activate your Python virtual environment and install the required packages if needed.\n",
    "- Required packages; torch, torchvision, accelerate or torchrun, tensorboard or wandb for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell: Check and List Available GPUs\n",
    "import torch\n",
    "\n",
    "print(\"Torch version;\", torch.__version__)\n",
    "print(\"CUDA available;\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs;\", torch.cuda.device_count())\n",
    "print(\"GPU Names;\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab720d",
   "metadata": {},
   "source": [
    "> **Troubleshooting:** If no GPUs are listed, review your Azure VM or local CUDA setup. Consult the Week 14 troubleshooting handout.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Task; Refactor Single-GPU Model for Multi-GPU Training\n",
    "\n",
    "- Update your training script; wrap your model with `torch.nn.DataParallel` or migrate components to use `torchrun` or `accelerate`.\n",
    "- DataParallel example is simplest but less flexible; accelerate or torchrun is preferred for real-world use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell: Sample Model Wrapping with DataParallel\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecaec3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Distributed Training with torchrun; Step-by-Step Guide\n",
    "\n",
    "- torchrun enables launching processes across multiple GPUs and nodes; part of PyTorch's distributed training toolkit.\n",
    "- Example launch command for 2 GPUs:\n",
    "  ```\n",
    "  torchrun --nproc_per_node=2 your_script.py\n",
    "  ```\n",
    "- Your training script must initialize the distributed environment and set the right device per process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell: Example Main Block for Distributed Setup\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    print(f\"Running on GPU {local_rank}\")\n",
    "\n",
    "    # Your model, dataloader, optimizer, etc.\n",
    "    # model = ...\n",
    "    # model.cuda(local_rank)\n",
    "    # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23282a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Integrating Experiment Tracking; Logging Key Metrics\n",
    "\n",
    "- Integrate tracking tools; use TensorBoard or Weights&Biases (wandb) for logging loss, accuracy, and runtime metrics.\n",
    "- Set up dashboard or log file for easy comparison between single-GPU and multi-GPU results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878813c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell: Simple TensorBoard Logging Example\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/distributed_demo')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = ...\n",
    "    val_accuracy = ...\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9fb04",
   "metadata": {},
   "source": [
    "> **Exercise:** Extend the logging code to record additional metrics such as GPU memory usage, throughput (images/sec), or gradient norm.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Exercise; Experiment Tracking with Weights&Biases (Optional Industry Tool)\n",
    "\n",
    "- Log in or create a wandb account.\n",
    "- Add wandb tracking lines to your script, logging all hyperparameters and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell: wandb Integration Example\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"multi_gpu_demo\")\n",
    "wandb.config.update({\"epochs\": epochs, \"batch_size\": batch_size})\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = ...\n",
    "    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch})\n",
    "# When training finishes\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e57c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Interpreting Results; Comparing Performance\n",
    "\n",
    "- Collect run-time, memory utilization, accuracy, and other metrics from both single-GPU and multi-GPU jobs.\n",
    "- Use experiment tracking dashboards to visualize differences.\n",
    "- Typical outcomes; multi-GPU speeds up training, but may have startup overhead or reduced scaling efficiency depending on model and batch size.\n",
    "\n",
    "> **Discussion:** Why might distributed training not yield a perfect linear speedup? How can batch size and I/O become bottlenecks?\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Prepare and Present Results; Assessment Expectations\n",
    "\n",
    "- Create a summary table or graph comparing single-GPU and multi-GPU performance for your model.\n",
    "- Document experiment parameters; number of GPUs, model hyperparameters, major differences in setup.\n",
    "- Use screenshots from TensorBoard/wandb dashboards to support your analysis.\n",
    "- Write a brief reflection; What worked well? What were the main bottlenecks? How might these be addressed in a production environment?\n",
    "\n",
    "> **Submission checklist;**\n",
    "> - Code with correct multi-GPU setup.\n",
    "> - Experiment tracking (TensorBoard/wandb) screenshots.\n",
    "> - Table or graph with performance comparison.\n",
    "> - Short reflection (100-200 words).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Industry Best Practices; Troubleshooting and Quality Assurance\n",
    "\n",
    "- Test your code by running short jobs before scaling up; check logs for silent failures.\n",
    "- Reproducibility; make random seeds and environment settings explicit.\n",
    "- Document any problems and how you solved them.\n",
    "- Share your findings as you would with a team; clear code, tracked results, concise summary.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Reflection and Next Steps\n",
    "\n",
    "- Project presentation in Week 18; practice explaining your workflow and interpreting results for both technical and non-technical audiences.\n",
    "- Review industry checklists for model monitoring, scaling, and reporting before final submission.\n",
    "- Prepare 1-2 questions about distributed training or experiment tracking for class discussion.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Distributed training enables scalable deep learning on modern hardware; critical for applied ML roles.\n",
    "- Experiment tracking is fundamental for reliable, reproducible, and industry-standard workflows.\n",
    "- Your Week 17 project is an integrated demonstration of these advanced, workplace-ready skills."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
