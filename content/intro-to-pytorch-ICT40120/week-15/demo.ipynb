{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d0b37c",
   "metadata": {},
   "source": [
    "# Week 15; Distributed Training Fundamentals; Multi-GPU PyTorch Jobs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand how distributed training enables scalable, efficient machine learning for real-world AI and data science projects\n",
    "- Learn to set up, launch, and monitor distributed jobs using torchrun and accelerate on multi-GPU systems\n",
    "- Gain hands-on experience adapting existing PyTorch scripts for multi-GPU use\n",
    "- Monitor distributed training jobs with industry-standard experiment tracking tools\n",
    "- Prepare for final projects and industry requirements in ML engineering and cloud-based AI deployment\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Agenda\n",
    "\n",
    "1. Introduction to distributed training and data parallelism in industry workflows\n",
    "2. Multi-GPU setup and environment checks\n",
    "3. Running distributed jobs with torchrun; PyTorch DataParallel and DistributedDataParallel\n",
    "4. Accelerate for simplified multi-GPU workflows\n",
    "5. Experiment tracking and monitoring\n",
    "6. Hands-on challenges and troubleshooting\n",
    "7. Reflection and preparation for assessment\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Industry Context and Relevance\n",
    "\n",
    "- Modern AI projects and ML engineering roles require the ability to efficiently train models at scale\n",
    "- Distributed training is standard practice in cloud, HPC, and enterprise AI environments\n",
    "- Employers expect basic competency with tools like torchrun, accelerate, and experiment trackers\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Introduction to Distributed Training\n",
    "\n",
    "Distributed training allows heavy workloads (e.g. deep learning, large datasets) to be split across multiple GPUs or nodes; this is called data parallelism.\n",
    "\n",
    "Key components;\n",
    "- Each GPU processes a subset of the data\n",
    "- Gradients are synchronized between devices every step\n",
    "- Two common methods; DataParallel (easy, single machine), DistributedDataParallel (recommended for performance and scalability)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Environment Setup and Prerequisites\n",
    "\n",
    "Before running distributed jobs, ensure that your environment supports multiple GPUs and has the required packages.\n",
    "\n",
    "### Checking available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPUs detected: \", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}:\", torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba987c",
   "metadata": {},
   "source": [
    "- You should see at least two GPUs listed to proceed with multi-GPU demos\n",
    "\n",
    "### Install required libraries (if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install accelerate package if not already present\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d080b52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Running Distributed Jobs with torchrun\n",
    "\n",
    "PyTorch's torchrun utility is recommended for launching distributed jobs.\n",
    "\n",
    "### Example: Training a simple model using DistributedDataParallel\n",
    "\n",
    "Modify a basic training script to support DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    model = nn.Linear(10, 2).to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss().to(rank)\n",
    "    optimizer = optim.Adam(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Dummy input and target\n",
    "    inputs = torch.randn(32, 10).to(rank)\n",
    "    targets = torch.randn(32, 2).to(rank)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        outputs = ddp_model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939c348",
   "metadata": {},
   "source": [
    "- Save this script as `distributed_train.py` then launch with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0019c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "torchrun --nproc_per_node=2 distributed_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92119bc",
   "metadata": {},
   "source": [
    "- Replace `2` with the number of available GPUs\n",
    "\n",
    "---\n",
    "\n",
    "## 6. (Optional) Using accelerate for Simplified Multi-GPU Training\n",
    "\n",
    "Huggingface accelerate abstracts away some complexities of distributed PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94226115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model = nn.Linear(10, 2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)\n",
    "\n",
    "inputs = torch.randn(32, 10)\n",
    "targets = torch.randn(32, 2)\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = nn.MSELoss()(outputs, targets)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831a448",
   "metadata": {},
   "source": [
    "- To launch with accelerate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "accelerate config\n",
    "accelerate launch your_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40aac8",
   "metadata": {},
   "source": [
    "- Follow prompts for device and distributed options\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Monitoring Distributed Jobs; Experiment Tracking\n",
    "\n",
    "Experiment tracking is a standard part of MLOps and industry AI workflows.\n",
    "\n",
    "Here is an example using TensorBoard for basic scalar logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./runs/experiment1\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Training code...\n",
    "    train_loss = 0.2 * epoch  # Dummy loss value\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd2644",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- After running your training script, launch TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4db02",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c83c07",
   "metadata": {},
   "source": [
    "- View metrics in your browser\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Hands-on Challenge\n",
    "\n",
    "- Modify a simple model to use `DistributedDataParallel` or `accelerate` for multi-GPU training (see above examples)\n",
    "- Track training loss and another metric of your choice using TensorBoard or MLflow\n",
    "- Try running your script with at least 2 GPUs; take a screenshot of the TensorBoard dashboard with your results\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Troubleshooting Common Issues\n",
    "\n",
    "- Ensure all dependencies support CUDA and are version matched (PyTorch, CUDA, torchrun, accelerate)\n",
    "- NCCL backend is required for GPU; ensure driver compatibility and proper CUDA_VISIBLE_DEVICES settings\n",
    "- Use unique output directories for logs and checkpoints to avoid conflicts\n",
    "- Monitor GPU usage with `nvidia-smi` during training for resource bottlenecks\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Reflection and Assessment Preparation\n",
    "\n",
    "- What are the main benefits of distributed training in production AI?\n",
    "- Describe the steps required to prepare and launch a distributed PyTorch job\n",
    "- How does experiment tracking support MLOps standards in industry?\n",
    "- List two issues that might arise in multi-GPU training and their solutions\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary and Next Steps\n",
    "\n",
    "- Distributed training is essential for scaling ML workloads in cloud and HPC\n",
    "- torchrun and accelerate are industry tools for efficiently managing distributed jobs\n",
    "- Experiment tracking is a baseline industry requirement; practice integrating it into all projects\n",
    "- Next week; begin your final projects implementing these concepts, with support for both single and multi-GPU workflows"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
