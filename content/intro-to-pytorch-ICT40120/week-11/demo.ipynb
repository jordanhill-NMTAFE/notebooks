{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c715cb",
   "metadata": {},
   "source": [
    "# Week 11 Demo; Production ML Workflows and Experiment Tracking\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the purpose of experiment tracking in machine learning workflows\n",
    "- Set up and run MLflow or Weights & Biases to track PyTorch experiments\n",
    "- Document and interpret experiment results with tracking dashboards\n",
    "- Apply reproducibility, transparency, and responsible AI principles in real-world projects\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction; Why Track ML Experiments?\n",
    "\n",
    "- Machine learning development often involves running multiple experiments; without tracking, results can become disorganised or lost\n",
    "- Experiment tracking enables reproducibility, transparency, and compliance; critical for production environments and workplace standards\n",
    "- Industry requires clear model documentation, versioning, and clear audit trails for model behaviour and evaluation metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Overview of Experiment Tracking Tools\n",
    "\n",
    "- MLflow and Weights & Biases (W&B) are popular open-source tools for tracking ML experiments; both integrate with PyTorch and support cloud-based workflows\n",
    "- Tracking tools record configuration parameters, code versions, training metrics, results, and artefacts (such as model weights)\n",
    "- Allows comparison of different experiment runs and supports model versioning for deployment readiness\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hands-On Demo; Setting Up Tracking for PyTorch Experiments\n",
    "\n",
    "### Step 1; Environment Setup\n",
    "\n",
    "- Ensure Python, pip, and PyTorch are installed; install MLflow or W&B as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install MLflow\n",
    "# !pip install mlflow\n",
    "\n",
    "# Uncomment the following line to install Weights & Biases\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678dcfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2; Quickstart; Logging with MLflow\n",
    "\n",
    "- Import MLflow and set up a simple experiment tracking context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy experiment; simple linear model\n",
    "with mlflow.start_run(run_name=\"linear_regression_demo\"):\n",
    "    model = nn.Linear(10, 1)\n",
    "    mlflow.log_param(\"input_dim\", 10)\n",
    "    mlflow.log_param(\"output_dim\", 1)\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)\n",
    "    \n",
    "    # Simulate a metric (loss)\n",
    "    train_loss = 0.123\n",
    "    mlflow.log_metric(\"train_loss\", train_loss)\n",
    "    # Log a model file\n",
    "    torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "    mlflow.log_artifact(\"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d8679",
   "metadata": {},
   "source": [
    "- Run the above cell; MLflow will save parameters, metrics, and the model to its default directory\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3; View and Interpret Results\n",
    "\n",
    "- To start the local MLflow UI and explore your results, run in the terminal:\n",
    "\n",
    "```\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "- Open the URL provided and observe experiment runs, parameters, metrics, and artefacts\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative; Quickstart with Weights & Biases\n",
    "\n",
    "- Login to W&B (requires free account) and initialise a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af755cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()  # Single-time login in notebook\n",
    "\n",
    "run = wandb.init(project=\"pytorch_experiment_demo\")\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "\n",
    "# Simulate loss logging\n",
    "wandb.log({\"train_loss\": 0.123})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be6e75",
   "metadata": {},
   "source": [
    "- View your run at https://wandb.ai/; compare metrics, parameters, and visualizations in the W&B dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Documenting and Interpreting Results\n",
    "\n",
    "- Use tracking dashboards to visualise and compare multiple experiment runs; look for changes in loss, accuracy, or other metrics across different configurations\n",
    "- Document key findings and model configuration in your experiment report; note which run produced the best performance and why\n",
    "- Industry standard is to include: model version, data version, training parameters, results, and hyperparameter configurations in all model documentation\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Industry Example; Reproducibility and Compliance\n",
    "\n",
    "- Many organisations require all model training to be tracked and reproducible; this supports responsible AI and compliance with data governance regulations\n",
    "- Example; A government AI project must audit all model decisions; experiment tracking is essential for demonstrating transparency and repeatability\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Exercise\n",
    "\n",
    "- Task; Modify the provided PyTorch script to track an additional hyperparameter (e.g. batch size or optimizer type) and log validation accuracy after each epoch\n",
    "- Compare at least two runs with different parameters; use MLflow or W&B dashboard to interpret which setup performed best\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Troubleshooting and Best Practices\n",
    "\n",
    "- If the tracking dashboard does not show new runs, confirm your script is calling log_param, log_metric, or wandb.log correctly\n",
    "- Ensure unique run names or identifiers for each experiment\n",
    "- Always document code version and data source for reproducibility; this supports MLOps and industry deployment pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Reflection and Assessment Preparation\n",
    "\n",
    "- Questions for review; Why is experiment tracking critical in production ML pipelines? What information should be included in experiment logs for workplace compliance? How does structured experiment tracking prepare you for deployment and MLOps workflows?\n",
    "- Apply tracking workflows to upcoming assessments and projects; use experiment tracking to evidence and justify model development decisions\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary and Next Steps\n",
    "\n",
    "- This week; you set up, ran, and interpreted experiment tracking with MLflow/W&B; gained experience with documentation and reproducibility best practices\n",
    "- Next week; Focus on Cloud Automation and Autoscaling for ML workloads using Azure and Bash scripting; Experiment tracking will be integrated into cloud pipelines"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
