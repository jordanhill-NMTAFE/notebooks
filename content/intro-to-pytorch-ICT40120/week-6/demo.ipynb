{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507c6c69",
   "metadata": {},
   "source": [
    "# Week 6 Demo: Building a Simple Neural Network in PyTorch\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand PyTorch's autograd system for automatic differentiation\n",
    "- Define and construct a basic neural network using PyTorch modules and layers\n",
    "- Perform and visualize forward and backward passes\n",
    "- Diagnose common autograd and tensor errors and apply debugging best practices\n",
    "- Connect model components and workflows to real-world industry scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "Deep neural networks are the backbone of modern AI applications. This workshop shows how to define, train, and debug a simple neural network in PyTorch—covering essentials for building production-ready models and preparing for scalable GPU/cloud training.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28975310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure these libraries are installed in your environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check PyTorch and GPU availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac49caf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch Autograd Basics\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation system. It tracks operations on tensors with requires_grad=True, enabling flexible computation of gradients for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781dee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with autograd tracking enabled\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x * 3 + 2\n",
    "z = y.sum()\n",
    "print('Value of z:', z.item())\n",
    "z.backward()  # Perform backpropagation\n",
    "print('Gradient of x:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4229718",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "- Autograd tracks tensor operations to build a computation graph.\n",
    "- Calling .backward() computes all gradients needed for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Building a Simple Neural Network\n",
    "\n",
    "PyTorch neural networks are defined as subclasses of nn.Module. Layers are assembled in the constructor, and the forward method implements computation from input to output.\n",
    "\n",
    "### Industry Context\n",
    "In the workplace, modular, reusable PyTorch models accelerate experimentation and deployment in AI workflows—essential for production ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)       # Input to hidden\n",
    "        self.relu = nn.ReLU()                             # Activation\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)      # Hidden to output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the network\n",
    "net = SimpleNet(input_dim=2, hidden_dim=4, output_dim=1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf28ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Manual Forward and Backward Pass\n",
    "\n",
    "Trace the flow of a sample input through the network and understand how errors are propagated backward for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input and target\n",
    "input = torch.tensor([[1.0, -1.0]], requires_grad=True)\n",
    "target = torch.tensor([[0.5]])\n",
    "\n",
    "# Forward pass (prediction)\n",
    "output = net(input)\n",
    "print('Output:', output.item())\n",
    "\n",
    "# Calculate loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, target)\n",
    "print('Loss:', loss.item())\n",
    "\n",
    "# Backward pass (compute gradients)\n",
    "loss.backward()\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'Gradient for {name}: {param.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa697",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Common Debugging: Autograd and Shape Errors\n",
    "\n",
    "### Frequent Pitfalls\n",
    "- Mismatch in input/output shapes.\n",
    "- Forgetting requires_grad=True.\n",
    "- In-place operations that break the computation graph.\n",
    "\n",
    "### Example 1: Shape Mismatch Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_bad = torch.tensor([1.0, 2.0, 3.0])  # Wrong shape for input_dim=2\n",
    "    net(input_bad)\n",
    "except Exception as e:\n",
    "    print('Shape error:', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790796c",
   "metadata": {},
   "source": [
    "### Example 2: In-place Operation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tensor = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "try:\n",
    "    bad_tensor += 1  # In-place; can break autograd\n",
    "    bad_tensor.backward(torch.ones_like(bad_tensor))\n",
    "except Exception as e:\n",
    "    print('In-place op error:', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed080439",
   "metadata": {},
   "source": [
    "**Best Practice**; Always check tensor shapes and use out-of-place operations when working with autograd.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Hands-On Exercise\n",
    "\n",
    "- Define your own neural network class with different layer sizes\n",
    "- Pass a batch of random inputs through the network\n",
    "- Identify and fix any shape or type errors\n",
    "- Calculate loss and perform backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in dimensions and try different architectures, e.g., more layers\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNet, self).__init__()\n",
    "        # Your layers here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your computation here\n",
    "        pass\n",
    "\n",
    "# Create input and test your network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecf93b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Industry Case Study\n",
    "\n",
    "*Scenario*: You are tasked with building a network to predict equipment failure in a manufacturing plant using sensor data. The sensors provide multivariate numerical data daily.\n",
    "\n",
    "**Discussion**;\n",
    "- How would you structure your input layer?\n",
    "- What shapes and datatypes would you expect from your data pipeline?\n",
    "- What production issues could shape/autograd errors cause in a factory setting?\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Troubleshooting Checklist\n",
    "\n",
    "- Check input and output tensor shapes at each step\n",
    "- Verify requires_grad status for tensors that need gradients\n",
    "- Watch for unintended in-place operations (avoid .add_(), .relu_(), etc.)\n",
    "- Use loss.backward() only once per computational graph unless retain_graph=True\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Reflection and Assessment Prep\n",
    "\n",
    "- How does PyTorch’s autograd system simplify model training compared to manual differentiation?\n",
    "- What steps are needed to transition from a working script to a scalable, production-grade ML pipeline?\n",
    "- What are the key differences in debugging code locally, on cloud VMs, and on distributed GPUs?\n",
    "- Try writing unit tests for your SimpleNet to check input shapes and gradient flow\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary and Next Steps\n",
    "\n",
    "- You can now define, train, and debug basic neural networks with PyTorch\n",
    "- These foundations will power future work in GPU computing, distributed training, and production ML\n",
    "- Prepare by reviewing the official PyTorch tutorials on [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) and [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- Next week; You’ll apply these skills to real cloud GPU environments and manage scalable AI workflows in Azure\n",
    "\n",
    "---\n",
    "\n",
    "### End of Demo"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
